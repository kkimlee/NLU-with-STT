# Transformer
기존의 RNN기반의 Encoder, Decoder를 발전시킨 모델

## 전통적인 RNN 기반의 Encoder, Decoder
입력된 문장의 단어를 순차적으로 Encoding 함.
Encoding 결과로 Context vector가 계산됨.
Decoder는 Context vector를 입력받아 입력받은 문장을 번역.

하나의 고정된 크기의 벡터에 모든 정보를 압축하기 때문에 정보 손실이 발생함. 
RNN의 Vanishing Gradient 문제를 해결하지 못하였음.

입력 문장의 길이가 길어지면 품질이 매우 떨어짐.

### 어텐션 메커니즘
Decoder가 출력 문장을 예측할 때, Encoder에서 전체 입력 문장을 참고하는 방식.    
전체 입력 문장을 전부 다 동일한 비중으로 참고하는 것이 아닌, 해당 시점에서 예측해야하는 문장과 연관이 있는 입력 단어 부분에 집중하여 참고함.

#### 어텐션 함수
어텐션 함수는 Query에 대해서 모든 Key와의 유사도를 계산함.    
계산된 유사도는 키와 맵핑되어있는 Value에 반영됨.    
유사도가 반영된 Value를 모두 더해서 반환함.

Query는 디코더에서 
